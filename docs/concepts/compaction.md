---
summary: "Context window + compaction: how OpenClaw keeps sessions under model limits"
read_when:
  - You want to understand auto-compaction and /compact
  - You are debugging long sessions hitting context limits
title: "Compaction"
---

# Context Window & Compaction

Every model has a **context window** (max tokens it can see). Long-running chats accumulate messages and tool results; once the window is tight, OpenClaw **compacts** older history to stay within limits.

## What compaction is

Compaction **summarizes older conversation** into a compact summary entry and keeps recent messages intact. The summary is stored in the session history, so future requests use:

- The compaction summary
- Recent messages after the compaction point

Compaction **persists** in the sessionâ€™s JSONL history.

## Configuration

Use the `agents.defaults.compaction` setting in your `openclaw.json` to configure compaction behavior (mode, target tokens, etc.).
Compaction summarization preserves opaque identifiers by default (`identifierPolicy: "strict"`). You can override this with `identifierPolicy: "off"` or provide custom text with `identifierPolicy: "custom"` and `identifierInstructions`.

### Dedicated compaction model

By default, compaction uses the primary model. Some providers (e.g. CLI-based wrappers) may not support the summarization API, causing compaction to fail silently during auto-compaction. Set `agents.defaults.model.compact` to a dedicated provider/model for reliable compaction:

```json5
{
  agents: {
    defaults: {
      model: {
        primary: "google-gemini-cli/gemini-3-pro-preview",
        compact: "myapi/gemini-3-pro-preview",
      },
    },
  },
}
```

- Applies to both `/compact` and auto-compaction (context overflow).
- If the compact model fails, OpenClaw falls back to the primary model automatically.
- Compaction quality directly affects context accuracy, so pick a capable model.

## Auto-compaction (default on)

When a session nears or exceeds the modelâ€™s context window, OpenClaw triggers auto-compaction and may retry the original request using the compacted context.

Youâ€™ll see:

- `ðŸ§¹ Auto-compaction complete` in verbose mode
- `/status` showing `ðŸ§¹ Compactions: <count>`

Before compaction, OpenClaw can run a **silent memory flush** turn to store
durable notes to disk. See [Memory](/concepts/memory) for details and config.

## Manual compaction

Use `/compact` (optionally with instructions) to force a compaction pass:

```
/compact Focus on decisions and open questions
```

## Context window source

Context window is model-specific. OpenClaw uses the model definition from the configured provider catalog to determine limits.

## Compaction vs pruning

- **Compaction**: summarises and **persists** in JSONL.
- **Session pruning**: trims old **tool results** only, **in-memory**, per request.

See [/concepts/session-pruning](/concepts/session-pruning) for pruning details.

## OpenAI server-side compaction

OpenClaw also supports OpenAI Responses server-side compaction hints for
compatible direct OpenAI models. This is separate from local OpenClaw
compaction and can run alongside it.

- Local compaction: OpenClaw summarizes and persists into session JSONL.
- Server-side compaction: OpenAI compacts context on the provider side when
  `store` + `context_management` are enabled.

See [OpenAI provider](/providers/openai) for model params and overrides.

## Tips

- Use `/compact` when sessions feel stale or context is bloated.
- Large tool outputs are already truncated; pruning can further reduce tool-result buildup.
- If you need a fresh slate, `/new` or `/reset` starts a new session id.
